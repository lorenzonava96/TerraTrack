{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "R5x11YaIF9x2",
      "metadata": {
        "id": "R5x11YaIF9x2"
      },
      "source": [
        "<p>\n",
        "  <img src=\"https://raw.githubusercontent.com/lorenzonava96/TerraTrack/main/figures/logo.png\" alt=\"TerraTrack\" width=\"120\">\n",
        "</p>\n",
        "\n",
        "# TerraTrack ‚Äî Feature Tracking Workflow\n",
        "\n",
        "This notebook implements the **feature tracking component** of TerraTrack for detecting and monitoring slow-moving landslides using **Sentinel-2 imagery**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Bn8Xi8D5godG",
      "metadata": {
        "id": "Bn8Xi8D5godG"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <tr>\n",
        "    <td>\n",
        "      <a target=\"_blank\" href=\"https://github.com/lorenzonava96/TerraTrack/tree/main\">\n",
        "        <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
        "        View repository on GitHub\n",
        "      </a>\n",
        "    </td>\n",
        "    <td>\n",
        "      <a target=\"_blank\" href=\"https://github.com/lorenzonava96/TerraTrack/blob/main/notebooks/TerraTrack_v1.ipynb\" download>\n",
        "        <img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />\n",
        "        Download notebook\n",
        "      </a>\n",
        "    </td>\n",
        "    <td>\n",
        "      <img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />\n",
        "      <em>Paper (coming soon)</em>\n",
        "    </td>\n",
        "  </tr>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lNhlSzSOgmPb",
      "metadata": {
        "collapsed": true,
        "id": "lNhlSzSOgmPb"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/lorenzonava96/TerraTrack.git\n",
        "!pip uninstall earthengine-api -y\n",
        "!pip uninstall geemap -y\n",
        "!pip install -r TerraTrack/requirements.txt\n",
        "!pip install --upgrade earthengine-api geemap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LvPB1GpDrlGg",
      "metadata": {
        "id": "LvPB1GpDrlGg"
      },
      "outputs": [],
      "source": [
        "import rasterio\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "from skimage import exposure\n",
        "from datetime import datetime\n",
        "from scipy.ndimage import convolve\n",
        "from scipy.signal import medfilt\n",
        "from scipy.ndimage import gaussian_filter\n",
        "import ee\n",
        "import geemap\n",
        "from TerraTrack.src import *\n",
        "\n",
        "output_dir = 'outputs'\n",
        "\n",
        "ee.Authenticate(auth_mode='notebook')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MFJnYfDerItV",
      "metadata": {
        "id": "MFJnYfDerItV"
      },
      "source": [
        "## Initialize and Define Area of Interest (AoI)\n",
        "\n",
        "This section initializes a `Map` instance using the `geemap` library and centers it on a global view with a satellite basemap. The map will help define the Area of Interest (AOI).\n",
        "\n",
        "### Instructions:\n",
        "1. Use the map tools on the left to draw a single rectangular box defining your AOI.\n",
        "2. Use the globe icon on the top left to search for specific locations if needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "woZrRkTdL235",
      "metadata": {
        "id": "woZrRkTdL235"
      },
      "outputs": [],
      "source": [
        "# Create a Map instance\n",
        "Map = geemap.Map()\n",
        "\n",
        "# Add a satellite basemap for visualization (similar to Google Earth)\n",
        "Map.add_basemap('HYBRID')\n",
        "\n",
        "# Center the map globally\n",
        "Map.setCenter(0, 0, 2)  # Center on the world with zoom level 2\n",
        "\n",
        "# Display the map for user interaction\n",
        "Map"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FrpwDQ4r1sct",
      "metadata": {
        "id": "FrpwDQ4r1sct"
      },
      "source": [
        "### **Filtering and Processing Sentinel-2 Imagery**\n",
        "\n",
        "This block sets parameters to filter and process Sentinel-2 images.\n",
        "\n",
        "#### üîç Filters:\n",
        "- **Date range**: `START_YEAR`‚Äì`END_YEAR` (default: 2015‚Äì2025)\n",
        "- **Season**: January 1 ‚Äì December 30\n",
        "- **Cloud cover**: Max per tile via `CLOUD_COVER_MAX` (default: 50%)\n",
        "- **Cloud removal in ROI**: Enabled via `check_clouds`, excludes images with >5% clouds (`cloud_threshold`)\n",
        "- **Snow removal**: Enabled via `check_snow`, excludes images with >5% snow (`snow_threshold`)\n",
        "- **Final cutoff date**: Optional (`final_date`) for limiting analysis window\n",
        "\n",
        "#### üíß Sampling & Water Masking:\n",
        "- **Images per year**: `N_PER_YEAR` (default: 5)\n",
        "- **Water masking**: Enabled via `mask_water`, uses NDWI\n",
        "\n",
        "#### üì§ Outputs:\n",
        "- `final_collection`: Filtered Sentinel-2 image collection\n",
        "- `morpho`: Terrain data (DEM, slope, aspect)\n",
        "\n",
        "üí° **Tips**:\n",
        "- Tweak `cloud/snow thresholds` to trade off quality vs. coverage\n",
        "- Use `final_date` for back-analysis or scenario-based studies\n",
        "- Increase `N_PER_YEAR` for higher temporal resolution\n",
        "- Use `check_snow` in snowy terrain to reduce noise\n",
        "\n",
        "‚û°Ô∏è The block prints the number of selected images and acquisition dates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oMW3qkYorlX3",
      "metadata": {
        "id": "oMW3qkYorlX3"
      },
      "outputs": [],
      "source": [
        "# Define filtering parameters\n",
        "SUMMER_START = '-01-01'       # Start of seasonal filter (MM-DD)\n",
        "SUMMER_END = '-12-30'         # End of seasonal filter (MM-DD)\n",
        "START_YEAR = 2015             # First year to include\n",
        "END_YEAR = 2025               # Last year to include\n",
        "final_date = None     # Optional: limit images up to this date (YYYY-MM-DD)\n",
        "\n",
        "# Image and mask filtering\n",
        "CLOUD_COVER_MAX = 50          # Max allowable cloud cover per tile (%)\n",
        "N_PER_YEAR = 5               # Max number of images sampled per year\n",
        "mask_water = False            # Enable NDWI-based water masking\n",
        "check_clouds = True           # Remove images with too much cloud in the ROI\n",
        "cloud_threshold = 5           # Max allowable cloud cover in ROI (%)\n",
        "check_snow = True            # Remove images with excessive snow in ROI\n",
        "snow_threshold = 5            # Max allowable snow cover in ROI (%)\n",
        "\n",
        "# Define region of interest from drawn features\n",
        "roi = ee.FeatureCollection(Map.draw_features).geometry()\n",
        "\n",
        "# Process Sentinel-2 imagery and extract terrain data\n",
        "final_collection, morpho = process_sentinel2_data(\n",
        "    roi, START_YEAR, END_YEAR, SUMMER_START, SUMMER_END,\n",
        "    cloud_cover_max=CLOUD_COVER_MAX,\n",
        "    n_per_year=N_PER_YEAR,\n",
        "    mask_water=mask_water,\n",
        "    check_clouds=check_clouds,\n",
        "    cloud_threshold=cloud_threshold,\n",
        "    check_snow=check_snow,\n",
        "    snow_threshold=snow_threshold,\n",
        "    final_date=final_date\n",
        ")\n",
        "\n",
        "# Print the size of the filtered image collection\n",
        "print(\"Final collection size:\", final_collection.size().getInfo())\n",
        "\n",
        "# Print acquisition dates of selected images\n",
        "dates = final_collection.aggregate_array('system:time_start') \\\n",
        "    .map(lambda d: ee.Date(d).format('YYYY-MM-dd')).getInfo()\n",
        "\n",
        "print(\"Acquisition dates in final_collection:\")\n",
        "for d in dates:\n",
        "    print(d)\n",
        "\n",
        "print(\"Processing completed. Final collection and morpho are ready to be downloaded.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ORwa66YM-HMA",
      "metadata": {
        "id": "ORwa66YM-HMA"
      },
      "source": [
        "### **Downloading Processed Data**\n",
        "\n",
        "This block automates export of Sentinel-2 and terrain products:\n",
        "\n",
        "- **Sentinel-2 Composite**:  \n",
        "  - Creates a stack of the **B8 (NIR) band**  \n",
        "  - Output: `S2_Composite.tif`\n",
        "\n",
        "- **Morphological Data**:  \n",
        "  - Includes **DEM, Slope, and Aspect**  \n",
        "  - Output: `morpho.tif`\n",
        "\n",
        "- **Metadata**:  \n",
        "  - Includes image IDs, dates, cloud cover, orbit info, and tile IDs  \n",
        "  - Output: `S2_Metadata.csv`\n",
        "\n",
        "üí° No user input needed ‚Äî the download is fully automated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "B63tSIo3rlar",
      "metadata": {
        "id": "B63tSIo3rlar"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Set up directory for output files\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Generate a composite image by stacking the B8 band across the image collection\n",
        "composite_image = final_collection.select('B8').toBands()\n",
        "composite_output_path = os.path.join(output_dir, 'S2_Composite.tif')\n",
        "\n",
        "# Download S2 Composite\n",
        "geemap.download_ee_image(composite_image, composite_output_path, scale=None, crs=None, region=roi.getInfo())\n",
        "print(\"Composite image downloaded.\")\n",
        "\n",
        "# Download morpho\n",
        "composite_output_path = os.path.join(output_dir, 'morpho.tif')\n",
        "geemap.download_ee_image(morpho, composite_output_path, scale=None, crs=None, region=roi.getInfo())\n",
        "\n",
        "print(\"DEM, Slope, and Aspect downloaded.\")\n",
        "\n",
        "# Aggregate metadata arrays for the entire collection in a single query\n",
        "image_ids = final_collection.aggregate_array('system:index').getInfo()\n",
        "dates = final_collection.aggregate_array('system:time_start').map(lambda t: ee.Date(t).format('YYYY-MM-dd')).getInfo()\n",
        "cloud_covers = final_collection.aggregate_array('CLOUDY_PIXEL_PERCENTAGE').getInfo()\n",
        "orbits = final_collection.aggregate_array('SENSING_ORBIT_NUMBER').getInfo()\n",
        "tile_ids = final_collection.aggregate_array('MGRS_TILE').getInfo()\n",
        "\n",
        "# Combine into a dictionary for DataFrame creation\n",
        "metadata = {\n",
        "    'Image_ID': image_ids,\n",
        "    'Date': dates,\n",
        "    'Cloud_Cover': cloud_covers,\n",
        "    'Orbit_Number': orbits,\n",
        "    'Tile_ID': tile_ids\n",
        "}\n",
        "\n",
        "# Convert to DataFrame and save to CSV\n",
        "metadata_df = pd.DataFrame(metadata)\n",
        "# Clean the ID column\n",
        "metadata_df[\"Image_ID\"] = metadata_df[\"Image_ID\"].str.replace(r'^.*?(\\d{8}T\\d{6}.*)$', r'\\1', regex=True)\n",
        "metadata_csv_path = os.path.join(output_dir, 'S2_Metadata.csv')\n",
        "metadata_df.to_csv(metadata_csv_path, index=False)\n",
        "\n",
        "print(\"Metadata saved to CSV file.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zUcIuMVfOmbR",
      "metadata": {
        "id": "zUcIuMVfOmbR"
      },
      "source": [
        "### **Processing Sentinel-2 Composite**\n",
        "This block processes the **Sentinel-2 composite image**, allowing the user to choose how images are selected:  \n",
        "\n",
        "- **Selection Method (`selection_method`)**:  \n",
        "  - `\"manual\"` ‚Äì User manually selects images for the composite.  \n",
        "  - `\"auto\"` ‚Äì Keeps all the images.  \n",
        "\n",
        "üí° **Tip:** Use `\"auto\"` for quick processing, or `\"manual\"` for full control over image selection.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1xeuoEG2loba",
      "metadata": {
        "id": "1xeuoEG2loba"
      },
      "outputs": [],
      "source": [
        "process_composite_image(output_dir=output_dir, selection_method=\"auto\")  #auto, manual"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "q61Xzbpb-wS6",
      "metadata": {
        "id": "q61Xzbpb-wS6"
      },
      "source": [
        "### **Preprocessing Sentinel-2 Composite**\n",
        "This block **opens, processes, and prepares** the Sentinel-2 composite for analysis.  \n",
        "\n",
        "- **Processing Method (`method`)**:  \n",
        "  - `\"cross_corr\"` (**Recommended**) ‚Äì Provides accurate alignment with minimal noise.  \n",
        "  - `\"optical_flow\"` ‚Äì Can be very noisy and is generally not recommended.  \n",
        "\n",
        "- **Steps**:  \n",
        "  1. Loads the **S2 composite image** (`S2_Composite_Filtered_8bit.tif`).  \n",
        "  2. Reads and transposes the stack for processing.  \n",
        "  3. **Preprocesses the stack** using the selected method.  \n",
        "  4. Transposes the stack for visualization.  \n",
        "\n",
        "üí° **Tip:** `\"cross_corr\"` is the best option for reliable results. `\"optical_flow\"` is prone to noise and should only be used for specific cases.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d93cb75",
      "metadata": {
        "id": "2d93cb75",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Set feature tracking method: 'cross_corr' (NCC, PCC) or 'optical_flow'\n",
        "method = 'cross_corr'\n",
        "\n",
        "# Define preprocessing parameters\n",
        "preprocess_params = {\n",
        "    \"method\": method\n",
        "}\n",
        "\n",
        "# Load the composite image stack (uint8, filtered)\n",
        "with rasterio.open(f'{output_dir}/S2_Composite_Filtered_8bit.tif') as src:\n",
        "    num_bands = src.count\n",
        "    print(f\"üì¶ Number of bands in composite: {num_bands}\")\n",
        "\n",
        "    # Read the full stack: shape (bands, height, width)\n",
        "    orig = src.read()\n",
        "\n",
        "# Preprocess the image stack (e.g., normalize, filter)\n",
        "# Input shape: (bands, H, W) ‚Üí returns (H, W, bands)\n",
        "preprocessed_stack = preprocess_image_stack(orig, preprocess_params)\n",
        "\n",
        "# Transpose for visualization/processing: (H, W, bands)\n",
        "orig = np.transpose(orig, (1, 2, 0))\n",
        "print(f\"‚úÖ Original stack shape:     {orig.shape}\")\n",
        "print(f\"‚úÖ Preprocessed stack shape: {preprocessed_stack.shape}\")\n",
        "\n",
        "# Build a mask: 1 if a pixel is 0 in ANY band (e.g., masked out earlier), else 0\n",
        "zero_mask = np.any(orig == 0, axis=2).astype(int)\n",
        "\n",
        "# Apply the mask across the full stack: set masked pixels to NaN\n",
        "orig_masked = np.where(zero_mask[..., np.newaxis] == 1, np.nan, orig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MPi2Uca9vn9K",
      "metadata": {
        "id": "MPi2Uca9vn9K"
      },
      "outputs": [],
      "source": [
        "### PLOT ###\n",
        "\n",
        "# Define the number of images to display\n",
        "num_images = min(5, orig.shape[2])\n",
        "\n",
        "# Create subplots\n",
        "fig, axes = plt.subplots(num_images, 2, figsize=(15, 2 * num_images))\n",
        "\n",
        "for i in range(num_images):\n",
        "    # Original image\n",
        "    axes[i, 0].imshow(orig[:, :, i], cmap='gray')\n",
        "    axes[i, 0].set_title(f'Original Image {i+1}')\n",
        "    axes[i, 0].axis('off')\n",
        "\n",
        "    # Processed image\n",
        "    axes[i, 1].imshow(preprocessed_stack[:, :, i], cmap='gray')\n",
        "    axes[i, 1].set_title(f'Processed Image {i+1}')\n",
        "    axes[i, 1].axis('off')\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11VevUya_uAG",
      "metadata": {
        "id": "11VevUya_uAG"
      },
      "source": [
        "### **Defining Date Pairs for Analysis**\n",
        "This block selects **valid date pairs** from metadata based on a **user-defined time separation range** (`min_separation` & `max_separation`).  \n",
        "\n",
        "- **User-Defined Time Constraints**:  \n",
        "  - **Minimum separation (`min_separation`)**: Default `1 year` ‚Üí Captures **faster changes**, but may increase noise.  \n",
        "  - **Maximum separation (`max_separation`)**: Default `5 years` ‚Üí Detects **slower movements**, but increases the chance of weak matches.   \n",
        "\n",
        "üí° **User Tip:**  \n",
        "- **Shorter separations** ‚Üí **Better matches**, but may not capture slow processes.  \n",
        "- **Longer separations** ‚Üí **Detects gradual changes**, but can result in **weaker correlations**.  \n",
        "üîπ Adjust `min_separation` and `max_separation` **based on your study's needs**.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f54bc7c7",
      "metadata": {
        "id": "f54bc7c7"
      },
      "outputs": [],
      "source": [
        "min_separation = 1 # in years\n",
        "max_separation = 5 # in years\n",
        "\n",
        "metadata_path = f\"{output_dir}/Updated_Metadata.csv\"\n",
        "\n",
        "dat1, dat2, separation, datax = define_date_pairs(metadata_path, min_separation=min_separation, max_separation=max_separation)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bd7f772",
      "metadata": {
        "id": "8bd7f772"
      },
      "source": [
        "# **Feature Tracking**\n",
        "\n",
        "### **Test on a Single Image Pair**\n",
        "This block tests tracking parameters on one image pair (`img1`: band 1, `img2`: band 35) to fine-tune before full-scale processing. Time separation between images affects detectability.\n",
        "\n",
        "---\n",
        "\n",
        "## **Core Parameters**\n",
        "- **Method:** `\"block_matching\"` (**recommended**), `\"optical_flow\"` (noisier)\n",
        "- **Function:** `\"fft_ncc\"` (**recommended**), `\"phase_cross_corr\"`, `\"mean_optical_flow\"`\n",
        "- **Subpixel:** `\"parabolic\"` (**fast & accurate**), `\"quadratic\"` (slower but precise)\n",
        "- **Block Size / Overlap:**\n",
        "  - Small ‚Üí more detail, slower\n",
        "  - Large ‚Üí faster, less precision\n",
        "  - High overlap (e.g. `0.9`) improves results but increases runtime\n",
        "\n",
        "---\n",
        "\n",
        "## **Filtering (Apply After Full Stack Processing)**\n",
        "- **Magnitude filter:** Acceptable range (`0.1‚Äì5`)\n",
        "- **Zero masking:** Excludes pre-masked areas\n",
        "- **Deviation filter:** Removes statistical outliers\n",
        "- **Angular coherence:** Keeps vectors within `‚â§50¬∞`\n",
        "- **PKR threshold (`1.3`):** Filters weak correlation peaks\n",
        "- **SNR threshold (`3`):** Retains confident matches\n",
        "\n",
        "---\n",
        "\n",
        "## **Performance Tips**\n",
        "- **Speed depends on** image size, block size, overlap, filters\n",
        "- **Too slow?** ‚Üí Increase block size, reduce overlap  \n",
        "- **Too noisy?** ‚Üí Raise SNR, enable angular coherence\n",
        "\n",
        "---\n",
        "\n",
        "## **Outputs**\n",
        "- Displacement (`u`, `v`), feature points, `pkr`, `snr`, and processing time for one image pair.\n",
        "\n",
        "‚öôÔ∏è Use this to validate settings before scaling up."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f92475de",
      "metadata": {
        "id": "f92475de"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# --- Select Image Pair from Stack ---\n",
        "# (Band indices: 0-based)\n",
        "img1 = preprocessed_stack[:, :, 1]\n",
        "img2 = preprocessed_stack[:, :, 40]\n",
        "\n",
        "# --- Feature Tracking Parameters ---\n",
        "method = 'block_matching'            # 'block_matching' or 'optical_flow'\n",
        "match_func = 'fft_ncc'               # 'fft_ncc', 'phase_cross_corr', or 'mean_optical_flow'\n",
        "subpixel_method = 'parabolic'        # 'center_of_mass', 'parabolic', 'quadratic'\n",
        "\n",
        "block_size = 16\n",
        "overlap = 0.6\n",
        "\n",
        "# --- Displacement Limits ---\n",
        "subpixel_precision = 0.1\n",
        "max_displacement = block_size - 1\n",
        "\n",
        "# --- Filtering Options ---\n",
        "filter_params = {\n",
        "    \"apply_magnitude_filter\": True,\n",
        "    \"min_magnitude\": subpixel_precision,\n",
        "    \"max_magnitude\": 5,\n",
        "\n",
        "    \"apply_zero_mask_filter\": False,\n",
        "    \"apply_deviation_filter\": False,\n",
        "    \"std_factor\": 2.5,\n",
        "\n",
        "    \"apply_remove_median_displacement\": True,\n",
        "    \"apply_median_filter_step\": False,\n",
        "    \"filter_size\": 5,\n",
        "\n",
        "    \"apply_angular_coherence_filter\": False,\n",
        "    \"angular_threshold\": 50,\n",
        "    \"smoothing_sigma\": 1,\n",
        "\n",
        "    \"apply_erratic_displacement_filter\": False,\n",
        "    \"neighborhood_size\": 20,\n",
        "    \"deviation_threshold\": 2.0,\n",
        "\n",
        "    \"apply_pkr_filter\": False,\n",
        "    \"pkr_threshold\": 1.3,\n",
        "\n",
        "    \"apply_snr_filter\": False,\n",
        "    \"snr_threshold\": 3,\n",
        "}\n",
        "\n",
        "# --- Run Displacement Analysis ---\n",
        "start_time = time.time()\n",
        "\n",
        "u, v, feature_points, pkr, snr = displacement_analysis(\n",
        "    img1=img1,\n",
        "    img2=img2,\n",
        "    method=method,\n",
        "    block_size=block_size,\n",
        "    overlap=overlap,\n",
        "    match_func=match_func,\n",
        "    subpixel_method=subpixel_method,\n",
        "    zero_mask=zero_mask,\n",
        "    filter_params=filter_params,\n",
        "    plot=True,\n",
        "    arrow_scale=0.1\n",
        ")\n",
        "\n",
        "full_feature_points = feature_points  # Store full-resolution grid\n",
        "\n",
        "# --- Timing ---\n",
        "elapsed_time = time.time() - start_time\n",
        "print(f\"BLOCK MATCHING ‚Äî Process completed in {elapsed_time:.2f} seconds (1 pair).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BvmuPi1O1qiJ",
      "metadata": {
        "id": "BvmuPi1O1qiJ"
      },
      "outputs": [],
      "source": [
        "# Estimate total processing time for all image pairs\n",
        "num_pairs = len(separation)  # List of valid date pairs (from earlier step)\n",
        "estimated_total_time = elapsed_time * num_pairs / 60  # in minutes\n",
        "\n",
        "print(f\"‚è±Ô∏è Estimated total processing time for {num_pairs} pairs: {estimated_total_time:.1f} minutes (with parallel computing it'll be faster)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4JO8n1i36sBu",
      "metadata": {
        "id": "4JO8n1i36sBu"
      },
      "source": [
        "### **Processing Image Pairs for Displacement Analysis**\n",
        "\n",
        "This block runs **feature tracking** across multiple image pairs using the selected method.\n",
        "\n",
        "---\n",
        "\n",
        "## üö® Recommendation\n",
        "**Disable all filters at this stage.**  \n",
        "Apply filtering **after processing all pairs** to avoid inconsistent results. Early filtering may discard useful data ‚Äî it's better to clean up with post-processing.\n",
        "\n",
        "---\n",
        "\n",
        "## üõ† Output\n",
        "- Displacement vectors (`u`, `v`)\n",
        "- Feature points\n",
        "- Quality metrics (`snr`, `pkr`, etc.)\n",
        "- Prints `\"FINALLY DONE!\"` when complete ‚Äî time for a caff√®.\n",
        "\n",
        "üí° **Tips**\n",
        "- Adjust **block size** and **overlap** to manage speed vs. accuracy\n",
        "- Apply filters like angular coherence or magnitude thresholds **afterward**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c689b76",
      "metadata": {
        "id": "9c689b76",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# FT parameters\n",
        "method = 'block_matching' # block_matching, optical_flow\n",
        "match_func = 'fft_ncc' # fft_ncc, phase_cross_corr, mean_optical_flow\n",
        "subpixel_method='parabolic' # center_of_mass, quadratic, parabolic\n",
        "block_size = 16\n",
        "overlap = 0.6\n",
        "\n",
        "# Compute minimum and maximum displacement\n",
        "subpixel_precision = 0.1  # Typical subpixel precision for fft_ncc\n",
        "max_displacement = block_size - 1  # Maximum displacement limited by block size\n",
        "\n",
        "# Filter parameters must be all off here\n",
        "filter_params = {\n",
        "    \"apply_magnitude_filter\": False,\n",
        "    \"min_magnitude\": subpixel_precision,  # Automatically set based on subpixel precision\n",
        "    \"max_magnitude\": max_displacement,  # Maximum displacement limited by block size\n",
        "    \"apply_zero_mask_filter\": False,\n",
        "    \"apply_deviation_filter\": False,  # Enable filtering by deviation from mean\n",
        "    \"std_factor\": 2.5,  # Allow deviation within n standard deviations\n",
        "    \"apply_remove_median_displacement\": False,  # Remove the overall median displacement\n",
        "    \"apply_median_filter_step\": False,  # Disable post-filtering with a median filter\n",
        "    \"filter_size\": 5,  # Use a 5x5 median filter if enabled\n",
        "    \"apply_angular_coherence_filter\": False,  # Enable angular coherence filtering\n",
        "    \"angular_threshold\": 50,  # Angular threshold (degrees)\n",
        "    \"smoothing_sigma\": 1,  # Smoothing parameter for angular coherence\n",
        "    \"apply_erratic_displacement_filter\": False,  # Enable erratic displacement filtering\n",
        "    \"neighborhood_size\": 20,  # Neighborhood size for erratic displacement filtering\n",
        "    \"deviation_threshold\": 2.0,  # Threshold for deviation from neighborhood median\n",
        "    'apply_pkr_filter': False,\n",
        "    'pkr_threshold': 1.3,\n",
        "    'apply_snr_filter': False,\n",
        "    'snr_threshold': 3,\n",
        "}\n",
        "\n",
        "# Run the processing function\n",
        "results = process_image_pairs(\n",
        "    dat1=dat1,\n",
        "    dat2=dat2,\n",
        "    datax=datax,\n",
        "    preprocessed_stack=preprocessed_stack,\n",
        "    zero_mask=zero_mask,\n",
        "    filter_params=filter_params,\n",
        "    method=method,\n",
        "    block_size=block_size,\n",
        "    overlap=overlap,\n",
        "    match_func=match_func,\n",
        "    subpixel_method=subpixel_method,\n",
        "    max_workers=12,\n",
        "    parallel=True)\n",
        "\n",
        "print('FINALLY DONE !')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "x7XTDQrl7a9n",
      "metadata": {
        "id": "x7XTDQrl7a9n"
      },
      "source": [
        "### **Saving Raw Feature Tracking (FT) Output**\n",
        "This block **saves and loads** the raw **displacement results** from feature tracking.  \n",
        "\n",
        "üö® **Important:**  \n",
        "- The function **does not overwrite existing files**.  \n",
        "- If re-running this block, **manually delete the existing output** to avoid conflicts.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4VQCqdpi2ua0",
      "metadata": {
        "id": "4VQCqdpi2ua0"
      },
      "outputs": [],
      "source": [
        "data = handle_predictions(\n",
        "    output_dir=output_dir,\n",
        "    method=method,\n",
        "    match_func=match_func,\n",
        "    results=results,\n",
        "    separation=separation,\n",
        "    orig=orig,\n",
        "    dat1=dat1,\n",
        "    dat2=dat2,\n",
        "    save=True,\n",
        "    load=True\n",
        ")\n",
        "\n",
        "# Access variables\n",
        "all_u = data['all_u']\n",
        "all_v = data['all_v']\n",
        "all_feature_points = data['all_feature_points']\n",
        "all_pkrs = data['all_pkrs']\n",
        "all_snrs = data['all_snrs']\n",
        "separation = data['separation']\n",
        "study_area_image = data['study_area_image']\n",
        "dat1 = data['dat1']\n",
        "dat2 = data['dat2']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "betNjykSnE3J",
      "metadata": {
        "id": "betNjykSnE3J"
      },
      "source": [
        "### **Pairwise Filtering**\n",
        "\n",
        "Refines feature tracking results by removing unreliable displacement vectors using several techniques.\n",
        "\n",
        "---\n",
        "\n",
        "## **Filters Applied**\n",
        "1. **Magnitude** ‚Äî Removes values outside `0.1` to `block_size - 1`\n",
        "2. **Zero Mask** ‚Äî Excludes invalid pixels (e.g., clouds, water) *(off by default)*\n",
        "3. **Deviation** ‚Äî Removes outliers beyond `2.5œÉ`\n",
        "4. **Median Shift** ‚Äî Removes overall median displacement\n",
        "5. **Angular Coherence** ‚Äî Keeps vectors within `50¬∞` of dominant direction\n",
        "6. **Erratic Motion** ‚Äî Removes local outliers (`20 px radius`, `2.0` threshold)\n",
        "7. **PKR Threshold** ‚Äî Filters weak matches (`> 1.5`)\n",
        "8. **SNR Threshold** ‚Äî Retains high-confidence points (`> 3`)\n",
        "\n",
        "---\n",
        "\n",
        "üí° **Tip**:  \n",
        "Adjust `snr`, `pkr`, or `deviation` thresholds if results are too sparse.\n",
        "\n",
        "üöÄ Leaves only high-confidence vectors for post-analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nmOeCkRmyY18",
      "metadata": {
        "id": "nmOeCkRmyY18"
      },
      "outputs": [],
      "source": [
        "# Run this after the cells above to set an alert sound when the processing is finished\n",
        "play_alert()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "148oY261m-_n",
      "metadata": {
        "id": "148oY261m-_n"
      },
      "outputs": [],
      "source": [
        "# --- Filtering Parameters ---\n",
        "# These control which vectors are retained or removed post-tracking\n",
        "filter_params = {\n",
        "    \"apply_magnitude_filter\": True,          # Remove displacements below/above thresholds\n",
        "    \"min_magnitude\": subpixel_precision,     # Minimum accepted motion\n",
        "    \"max_magnitude\": max_displacement,       # Maximum accepted motion (usually block_size - 1)\n",
        "\n",
        "    \"apply_zero_mask_filter\": False,         # Exclude pixels masked in any image (e.g., water/clouds)\n",
        "\n",
        "    \"apply_deviation_filter\": False,         # Remove vectors far from statistical mean\n",
        "    \"std_factor\": 2.5,                       # Standard deviation threshold for deviation filter\n",
        "\n",
        "    \"apply_remove_median_displacement\": True,  # Subtract overall median motion (e.g., camera jitter)\n",
        "\n",
        "    \"apply_median_filter_step\": False,       # Smooth vectors using a median filter\n",
        "    \"filter_size\": 5,                        # Size of median filter kernel (if used)\n",
        "\n",
        "    \"apply_angular_coherence_filter\": False, # Remove vectors that don‚Äôt align with dominant motion\n",
        "    \"angular_threshold\": 50,                 # Max angular deviation (degrees)\n",
        "    \"smoothing_sigma\": 1,                    # Angular coherence smoothing parameter\n",
        "\n",
        "    \"apply_erratic_displacement_filter\": False,  # Remove isolated, noisy vectors\n",
        "    \"neighborhood_size\": 20,                 # Radius for local filtering\n",
        "    \"deviation_threshold\": 2.0,              # Threshold for local deviation\n",
        "\n",
        "    \"apply_pkr_filter\": True,                # Remove low-quality matches using PKR\n",
        "    \"pkr_threshold\": 1.3,                    # Minimum accepted peak-to-residual ratio\n",
        "\n",
        "    \"apply_snr_filter\": True,                # Remove low-confidence vectors based on SNR\n",
        "    \"snr_threshold\": 3,                      # Minimum signal-to-noise ratio\n",
        "}\n",
        "\n",
        "filtered_all_u = []\n",
        "filtered_all_v = []\n",
        "filtered_all_feature_points = []\n",
        "\n",
        "for u, v, points, pkrs, snrs in zip(all_u, all_v, all_feature_points, all_pkrs, all_snrs):\n",
        "    # Convert to numpy arrays with proper numeric types\n",
        "    u = np.array(u, dtype=np.float64)\n",
        "    v = np.array(v, dtype=np.float64)\n",
        "    points = np.array(points, dtype=np.float64)\n",
        "    pkrs = np.array(pkrs, dtype=np.float64)\n",
        "    snrs = np.array(snrs, dtype=np.float64)\n",
        "\n",
        "    # Optionally, align pkrs and snrs with u if there is a mismatch.\n",
        "    if pkrs.shape[0] != u.shape[0]:\n",
        "        print(\"Aligning pkr values: original shape\", pkrs.shape, \"expected:\", u.shape)\n",
        "        pkrs = pkrs[:len(u)]\n",
        "    if snrs.shape[0] != u.shape[0]:\n",
        "        print(\"Aligning snr values: original shape\", snrs.shape, \"expected:\", u.shape)\n",
        "        snrs = snrs[:len(u)]\n",
        "\n",
        "    # Ensure zero_mask is an array if it exists (and has proper shape)\n",
        "    if zero_mask is not None:\n",
        "        zm = np.array(zero_mask)\n",
        "    else:\n",
        "        zm = None\n",
        "\n",
        "    # Try to apply filtering\n",
        "    try:\n",
        "        u_filtered, v_filtered, points_filtered = filter_displacements(\n",
        "            u, v, points, zm, pkr_values=pkrs, snr_values=snrs,\n",
        "            **filter_params\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(\"Error during filtering for one pair:\")\n",
        "        print(\"u:\", u)\n",
        "        print(\"v:\", v)\n",
        "        print(\"points:\", points)\n",
        "        print(\"pkrs:\", pkrs)\n",
        "        print(\"snrs:\", snrs)\n",
        "        raise e\n",
        "\n",
        "    filtered_all_u.append(u_filtered)\n",
        "    filtered_all_v.append(v_filtered)\n",
        "    filtered_all_feature_points.append(points_filtered)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "shZ_j7tbtYrN",
      "metadata": {
        "id": "shZ_j7tbtYrN"
      },
      "source": [
        "### **Median Displacement Computation**  \n",
        "Aggregates filtered displacements across all image pairs, computes **median motion values** (displacement, magnitude, and direction), and visualizes the refined displacement field over the study area.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OVf9mrzym-7_",
      "metadata": {
        "id": "OVf9mrzym-7_"
      },
      "outputs": [],
      "source": [
        "pixel_size = 10   # Pixel resolution in meters\n",
        "\n",
        "# Step 1: Accumulate Displacements\n",
        "displacement_data = accumulate_displacement(filtered_all_u, filtered_all_v, filtered_all_feature_points, separation)\n",
        "\n",
        "# Step 2: Calculate Median Displacement\n",
        "median_feature_points, median_u, median_v, median_magnitude, median_angles = calculate_median_displacement(displacement_data, pixel_size)\n",
        "\n",
        "plot_displacement_field(median_feature_points, median_u, median_v, median_magnitude, study_area_image, arrow_scale=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AT4g2-3Lt6Y8",
      "metadata": {
        "id": "AT4g2-3Lt6Y8"
      },
      "source": [
        "### **Final Displacement Filtering & Visualization**  \n",
        "Applies **terrain-based filters** (angular coherence, slope, aspect, clustering) to refine displacement data, removing weak or inconsistent movements. The DEM, slope, and aspect are resampled for accurate filtering, and the final **displacement field** is visualized.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3rgYSxGcm-2P",
      "metadata": {
        "id": "3rgYSxGcm-2P"
      },
      "outputs": [],
      "source": [
        "# Specify which filters to use & define parameters\n",
        "manual_threshold=None                   # If None, 95th percentile is used\n",
        "\n",
        "use_angular_coherence = True            # Use angular coherence filter\n",
        "angular_threshold_degrees = 30          # Angular threshold in degrees\n",
        "use_slope_filter = True                 # Skip slope filter\n",
        "min_slope_threshold = 5                 # Minimum slope in degrees\n",
        "use_aspect_filter = True                # Use aspect filter\n",
        "aspect_tolerance = 30                   # Allowable deviation from downslope direction in degrees\n",
        "use_clustering = True                   # Skip clustering filter\n",
        "clustering_params = (20, 9)             # Clustering parameters: (eps, min_samples)\n",
        "\n",
        "arrow_scale = 0.3                       # Scale of the vector arrow\n",
        "\n",
        "# Resample morpho image to match `study_area_image` shape\n",
        "morpho_path = f\"{output_dir}/morpho.tif\"\n",
        "output_path = f\"{output_dir}/resampled_morpho.tif\"\n",
        "resampled_morpho_path, resampled_transform = resample_morpho_to_match(study_area_image.shape, morpho_path, output_path)\n",
        "\n",
        "# Open and plot the resampled DEM, slope, and aspect\n",
        "with rasterio.open(resampled_morpho_path) as src:\n",
        "    resampled_dem = src.read(1)  # Band 1: Resampled DEM\n",
        "    resampled_slope = src.read(2)  # Band 2: Resampled slope\n",
        "    resampled_aspect = src.read(3)  # Band 3: Resampled aspect\n",
        "\n",
        "fil_median_feature_points, fil_median_u, fil_median_v, fil_median_magnitude = filter_final_map(\n",
        "    median_feature_points, median_u, median_v, median_magnitude, median_angles,\n",
        "    resampled_slope, resampled_aspect, study_area_image,\n",
        "    angular_threshold_degrees=angular_threshold_degrees, min_slope_threshold=min_slope_threshold,\n",
        "    aspect_tolerance=aspect_tolerance, smoothing_sigma=1, clustering_params=clustering_params,\n",
        "    use_angular_coherence=use_angular_coherence, use_slope_filter=use_slope_filter,\n",
        "    use_aspect_filter=use_aspect_filter, use_clustering=use_clustering, arrow_scale=arrow_scale,\n",
        "    manual_threshold=manual_threshold\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PsmhnOIkm-uD",
      "metadata": {
        "id": "PsmhnOIkm-uD"
      },
      "outputs": [],
      "source": [
        "# From cartesian to raster\n",
        "\n",
        "# filtered map\n",
        "fil_u_map, fil_v_map, fil_magnitude_map, fil_angle_map = create_raster_maps(\n",
        "    fil_median_feature_points, fil_median_u, fil_median_v, study_area_image, block_size, overlap\n",
        ")\n",
        "\n",
        "# full map\n",
        "u_map, v_map, magnitude_map, angle_map = create_raster_maps(\n",
        "    median_feature_points, median_u, median_v, study_area_image, block_size, overlap\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mqgYU9k1yYbv",
      "metadata": {
        "id": "mqgYU9k1yYbv"
      },
      "source": [
        "### **Generating & Saving Motion Colormap**  \n",
        "Creates a **refined displacement magnitude map**, overlays it on the composite image, and saves the result as `output_motion_colormap.tif` in GeoTIFF format, ensuring spatial alignment.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XBDIiRBgq5Ai",
      "metadata": {
        "id": "XBDIiRBgq5Ai"
      },
      "outputs": [],
      "source": [
        "processed_mask = process_mask(fil_magnitude_map)\n",
        "\n",
        "masked_magnitude = np.where(processed_mask, magnitude_map, np.nan)\n",
        "\n",
        "orig_path = f'{output_dir}/S2_Composite_Filtered_8bit.tif'\n",
        "output_path = f\"{output_dir}/output_motion_colormap.tif\"\n",
        "\n",
        "# Generate the magnitude map\n",
        "overlay_magnitude_map(orig[..., 0], masked_magnitude, block_size=block_size, overlap=overlap)\n",
        "\n",
        "save_as_geotiff(orig_path, output_path, masked_magnitude, block_size, overlap)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-mOJk3L7ycDA",
      "metadata": {
        "id": "-mOJk3L7ycDA"
      },
      "source": [
        "### **Time Series Reconstruction & Velocity Estimation**\n",
        "\n",
        "Reconstructs pixel-wise displacement time series and estimates average velocity from pairwise results.\n",
        "\n",
        "- **Method**:  \n",
        "  - `'weighted'` (**recommended**) ‚Äî weights displacements by SNR/PKR confidence  \n",
        "  - `'midpoint'` ‚Äî bins displacements by image-pair mid-date (equal weight)\n",
        "\n",
        "- **Temporal Resolution**:  \n",
        "  - `months_per_bin=4` ‚Üí Groups displacements into 4-month intervals  \n",
        "  - Use smaller bins (e.g., `1`) for finer time resolution\n",
        "\n",
        "- **Confidence Filtering**:  \n",
        "  - `min_snr=3`, `min_pkr=1.3` ‚Äî Filters out low-confidence displacements before time series reconstruction\n",
        "\n",
        "üß† **Tip**: Use `'weighted'` for better handling of irregular observation frequency and variable data quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "C4C3c4D3q-pG",
      "metadata": {
        "id": "C4C3c4D3q-pG"
      },
      "outputs": [],
      "source": [
        "displacement_data = accumulate_displacement_with_placeholders(\n",
        "    filtered_all_u, filtered_all_v, filtered_all_feature_points, separation, fil_median_feature_points, dat1, dat2, pixel_size, all_pkrs, all_snrs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vikVsfs3O1te",
      "metadata": {
        "id": "vikVsfs3O1te"
      },
      "outputs": [],
      "source": [
        "method = 'weighted' # 'weighted' or 'midpoint'\n",
        "months_per_bin = 4\n",
        "min_snr=3\n",
        "min_pkr=1.3\n",
        "\n",
        "velocity_estimates = estimate_velocity_time_series(displacement_data, method=method,\n",
        "                                  months_per_bin=months_per_bin,min_snr=min_snr, min_pkr=min_pkr)\n",
        "\n",
        "# Create velocity time series CSVs with median velocities for EW and SN components\n",
        "csv_data_we, csv_data_ns, csv_data_mag = prepare_csv_with_components(velocity_estimates, geotiff_path=f'{output_dir}/S2_Composite_Filtered_8bit.tif')\n",
        "\n",
        "# Save to CSV\n",
        "csv_data_we.to_csv(f'{output_dir}/velocity_time_series_we.csv', index=False)\n",
        "csv_data_ns.to_csv(f'{output_dir}/velocity_time_series_ns.csv', index=False)\n",
        "csv_data_mag.to_csv(f'{output_dir}/velocity_time_series_mag.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "P7AjvNNZykUi",
      "metadata": {
        "id": "P7AjvNNZykUi"
      },
      "outputs": [],
      "source": [
        "# Plot fastes points\n",
        "plot_fastest_points_components(csv_data_we, csv_data_ns, top_n=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Oi3gFudTymLi",
      "metadata": {
        "id": "Oi3gFudTymLi"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import rasterio\n",
        "from rasterio.warp import transform_bounds\n",
        "import pandas as pd\n",
        "\n",
        "# Paths to input files\n",
        "geotiff_path = f'{output_dir}/S2_Composite_Filtered_8bit.tif'\n",
        "csv_ew_path = f'{output_dir}/velocity_time_series_we.csv'\n",
        "csv_sn_path = f'{output_dir}/velocity_time_series_ns.csv'\n",
        "\n",
        "# Load CSVs\n",
        "csv_data_ew = pd.read_csv(csv_ew_path)\n",
        "csv_data_sn = pd.read_csv(csv_sn_path)\n",
        "\n",
        "# Combine WE and NS velocities into a single DataFrame\n",
        "csv_data_ew['mean_velocity'] = (csv_data_ew['median_velocity']**2 + csv_data_sn['median_velocity']**2)**0.5\n",
        "\n",
        "# Load GeoTIFF image and get transform & CRS\n",
        "with rasterio.open(geotiff_path) as src:\n",
        "    image = src.read(1)  # Read the first band\n",
        "    transform = src.transform\n",
        "    src_crs = src.crs\n",
        "\n",
        "    # Get image bounds in the native CRS\n",
        "    left = transform[2]\n",
        "    top = transform[5]\n",
        "    right = left + transform[0] * image.shape[1]\n",
        "    bottom = top + transform[4] * image.shape[0]\n",
        "\n",
        "    # Transform bounds to EPSG:84 (WGS84) so they match the CSV lat/lon\n",
        "    bounds_84 = transform_bounds(src_crs, 'EPSG:4326', left, bottom, right, top)\n",
        "\n",
        "# bounds_84 is in the order (min_lon, min_lat, max_lon, max_lat)\n",
        "xmin, ymin, xmax, ymax = bounds_84\n",
        "\n",
        "# Plot the image using the transformed bounds\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.imshow(image, cmap='gray', extent=[xmin, xmax, ymin, ymax])\n",
        "plt.title('Mean Velocity from Time Series Overlaid on Image')\n",
        "plt.xlabel('Longitude')\n",
        "plt.ylabel('Latitude')\n",
        "\n",
        "# Overlay mean velocity as scatter points (CSV coordinates assumed to be in EPSG:84)\n",
        "scatter = plt.scatter(\n",
        "    csv_data_ew['longitude'], csv_data_ew['latitude'],\n",
        "    c=csv_data_ew['mean_velocity'], cmap='coolwarm', s=15, edgecolor='k', alpha=0.7\n",
        ")\n",
        "plt.colorbar(scatter, label='Mean Velocity (m/year)')\n",
        "plt.grid()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LfThERh74RWa",
      "metadata": {
        "id": "LfThERh74RWa"
      },
      "source": [
        "### ‚¨áÔ∏è Downloading Outputs\n",
        "\n",
        "To download files directly from Colab to your computer. This creates a zip of the entire outputs folder and downloads it to device.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HssiSFAbypwW",
      "metadata": {
        "id": "HssiSFAbypwW"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Zip the folder\n",
        "shutil.make_archive(f'Terratrack_Results_{output_dir}', 'zip', 'outputs')\n",
        "\n",
        "# Download\n",
        "files.download(f'Terratrack_Results_{output_dir}.zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Sgbckihq0fku",
      "metadata": {
        "id": "Sgbckihq0fku"
      },
      "source": [
        "# Inverse Velocity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uCPKxzm50iKU",
      "metadata": {
        "id": "uCPKxzm50iKU"
      },
      "outputs": [],
      "source": [
        "failure_dates_list = plot_inverse_velocity_monthly(f\"{output_dir}/velocity_time_series_mag.csv\", output_folder=f\"{output_dir}/plots\", save_plots=False, n_points_for_fit_list=[4,5,6,7,8])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FUyOu0d70iIG",
      "metadata": {
        "id": "FUyOu0d70iIG"
      },
      "outputs": [],
      "source": [
        "failure_counts = failure_date_statistics(failure_dates_list)\n",
        "print(failure_counts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dUr5eSfF0h_9",
      "metadata": {
        "id": "dUr5eSfF0h_9"
      },
      "outputs": [],
      "source": [
        "plot_failure_distribution(failure_counts)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ee_ft",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
